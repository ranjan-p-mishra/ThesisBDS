{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4365bac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Linear Simulations: 100%|██████████| 1000/1000 [00:50<00:00, 19.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Simulation Results Summary (Mean ± Std Dev) ---\n",
      "                            mean       std\n",
      "naive_R2                0.995816  0.000083\n",
      "naive_beta_Z            1.396027  0.006894\n",
      "naive_beta_X            0.997019  0.009128\n",
      "naive_DP                2.735736  0.021247\n",
      "naive_CDP               0.153767  0.011365\n",
      "full_debias_R2          0.004996  0.000129\n",
      "full_debias_beta_Z_res  0.999849  0.009830\n",
      "full_debias_beta_X_res  0.699731  0.009976\n",
      "full_debias_DP          0.001198  0.000877\n",
      "full_debias_CDP         0.163022  0.010139\n",
      "ours_R2                 0.712808  0.004889\n",
      "ours_beta_Z             0.999849  0.009830\n",
      "ours_beta_X_res         0.699731  0.009976\n",
      "ours_DP                 1.275911  0.015924\n",
      "ours_CDP                0.017950  0.006248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from tqdm import tqdm \n",
    "\n",
    "N_SAMPLES = 10000  # Number of samples per simulation run\n",
    "N_SIMULATIONS = 1000  # Number of simulation runs to average results\n",
    "\n",
    "# True DGP parameters\n",
    "TRUE_BETA_0 = 1.0\n",
    "TRUE_BETA_S = 0.5\n",
    "TRUE_BETA_Z = 1.0\n",
    "TRUE_BETA_X = 0.7\n",
    "TRUE_DELTA_S = 0.8  # Z = delta_S * S + epsilon_Z\n",
    "TRUE_GAMMA_S = 0.6  # X = gamma_S * S + epsilon_X\n",
    "\n",
    "# Variances of error terms and S (using standard deviations here for clarity in generation)\n",
    "SIGMA_S = 1.0  # Standard deviation for S (S ~ N(0, 1^2))\n",
    "SIGMA_Z_EPS = 0.1  # Standard deviation for epsilon_Z\n",
    "SIGMA_X_EPS = 0.1  # Standard deviation for epsilon_X\n",
    "SIGMA_Y_EPS = 0.1  # Standard deviation for epsilon_Y\n",
    "\n",
    "# --- Helper functions ---\n",
    "\n",
    "def calculate_demographic_parity(y_pred, s_binary):\n",
    "    \"\"\"\n",
    "    Calculates Demographic Parity (DP) for continuous predictions or labels.\n",
    "    Assumes s_binary is already binarized (e.g., 0 and 1).\n",
    "    \"\"\"\n",
    "    s_unique = np.unique(s_binary)\n",
    "    if len(s_unique) < 2:\n",
    "        return 0.0  # Cannot calculate DP with less than 2 sensitive groups\n",
    "\n",
    "    # Get mean prediction for each sensitive group\n",
    "    mean_preds_group0 = np.mean(y_pred[s_binary == s_unique[0]])\n",
    "    mean_preds_group1 = np.mean(y_pred[s_binary == s_unique[1]])\n",
    "    \n",
    "    return np.abs(mean_preds_group1 - mean_preds_group0)\n",
    "\n",
    "\n",
    "def _compute_conditional_demographic_parity(y_pred_probs, s_binary, z, bin_size=20, min_group_samples=50):\n",
    "    \"\"\"\n",
    "    Computes Conditional Demographic Parity (CDP) based on admissible features Z.\n",
    "    This version is robust to sparse Z-bins by enforcing a minimum number of samples.\n",
    "    \"\"\"\n",
    "    # For a linear model, y_pred_probs are directly the predicted outcomes.\n",
    "    y_pred = y_pred_probs  # Use raw predicted outcomes for mean calculations\n",
    "\n",
    "    # Ensure s_binary and z are 1D arrays for consistency if they are single features\n",
    "    if s_binary.ndim > 1 and s_binary.shape[1] == 1: s_binary = s_binary.flatten()\n",
    "    if z.ndim > 1 and z.shape[1] == 1: z = z.flatten()\n",
    "\n",
    "    s_unique = np.unique(s_binary)\n",
    "    if len(s_unique) < 2:\n",
    "        return 0.0  # Cannot compute CDP if there's only one sensitive group after binarization\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'y_pred': y_pred,\n",
    "        's': s_binary  # Use the binarized S\n",
    "    })\n",
    "    \n",
    "    z_df_binned = pd.DataFrame(index=df.index)  # Create an empty DataFrame for binned Z features\n",
    "    \n",
    "    # Bin continuous features in Z to create discrete groups for conditioning\n",
    "    # Iterate over columns of the original z array if it's 2D, or just z if 1D\n",
    "    if z.ndim > 1:\n",
    "        for i, col_data in enumerate(z.T):  # Iterate over columns if z is 2D\n",
    "            col_name = f'z_col_{i}'\n",
    "            if pd.api.types.is_numeric_dtype(col_data) and len(np.unique(col_data)) > bin_size:\n",
    "                try:\n",
    "                    z_df_binned[col_name] = pd.qcut(col_data, bin_size, labels=False, duplicates='drop')\n",
    "                except ValueError:\n",
    "                    z_df_binned[col_name] = pd.cut(col_data, bin_size, labels=False, duplicates='drop')\n",
    "            else:\n",
    "                z_df_binned[col_name] = col_data\n",
    "    else:  # z is 1D\n",
    "        col_name = 'z_col_0'\n",
    "        if pd.api.types.is_numeric_dtype(z) and len(np.unique(z)) > bin_size:\n",
    "             try:\n",
    "                z_df_binned[col_name] = pd.qcut(z, bin_size, labels=False, duplicates='drop')\n",
    "             except ValueError:\n",
    "                z_df_binned[col_name] = pd.cut(z, bin_size, labels=False, duplicates='drop')\n",
    "        else:\n",
    "            z_df_binned[col_name] = z\n",
    "    \n",
    "    df = pd.concat([df, z_df_binned], axis=1)\n",
    "\n",
    "    max_cdp = 0.0\n",
    "    \n",
    "    # Group by the conditional features (Z)\n",
    "    # Iterate through each unique combination of binned conditional features (Z)\n",
    "    # Filter out combinations where any Z-column is NaN (due to duplicates='drop' on qcut/cut)\n",
    "    # This ensures we only group on valid Z bins.\n",
    "    valid_z_groups = df.dropna(subset=list(z_df_binned.columns))\n",
    "\n",
    "    for z_group_key, group_df in valid_z_groups.groupby(list(z_df_binned.columns)):\n",
    "        \n",
    "        # Check if all sensitive groups are present AND meet minimum sample size within this Z-group\n",
    "        all_s_present_and_sufficient = True\n",
    "        rates_for_s_groups = {}\n",
    "        \n",
    "        for s_val in s_unique:\n",
    "            s_sub_group = group_df[group_df['s'] == s_val]\n",
    "            if len(s_sub_group) < min_group_samples:  # Check minimum samples\n",
    "                all_s_present_and_sufficient = False\n",
    "                break\n",
    "            rates_for_s_groups[s_val] = s_sub_group['y_pred'].mean()\n",
    "        \n",
    "        if all_s_present_and_sufficient:\n",
    "            # Calculate all pairwise differences within this Z-group\n",
    "            for i in range(len(s_unique)):\n",
    "                for j in range(i + 1, len(s_unique)):\n",
    "                    s1 = s_unique[i]\n",
    "                    s2 = s_unique[j]\n",
    "                    \n",
    "                    rate1 = rates_for_s_groups[s1]\n",
    "                    rate2 = rates_for_s_groups[s2]\n",
    "                    \n",
    "                    max_cdp = max(max_cdp, abs(rate1 - rate2))\n",
    "    \n",
    "    return max_cdp\n",
    "\n",
    "\n",
    "# --- Data Generation Function ---\n",
    "def generate_data(n_samples):\n",
    "    # Generate S from a Normal distribution\n",
    "    s = np.random.normal(0, SIGMA_S, n_samples)\n",
    "    \n",
    "    # Generate Z and X based on S and independent errors\n",
    "    epsilon_z = np.random.normal(0, SIGMA_Z_EPS, n_samples)\n",
    "    z = TRUE_DELTA_S * s + epsilon_z\n",
    "    \n",
    "    epsilon_x = np.random.normal(0, SIGMA_X_EPS, n_samples)\n",
    "    x = TRUE_GAMMA_S * s + epsilon_x\n",
    "    \n",
    "    # Generate Y based on S, Z, X and independent error\n",
    "    epsilon_y = np.random.normal(0, SIGMA_Y_EPS, n_samples)\n",
    "    y = TRUE_BETA_0 + TRUE_BETA_S * s + TRUE_BETA_Z * z + TRUE_BETA_X * x + epsilon_y\n",
    "    \n",
    "    return s, z, x, y\n",
    "\n",
    "# --- Simulation Loop ---\n",
    "simulation_results = []\n",
    "\n",
    "# Calculate necessary variances and covariances based on DGP parameters for analytical expectations\n",
    "# These are population variances/covariances based on the DGP equations\n",
    "V_S = SIGMA_S**2\n",
    "V_Z_analytical = TRUE_DELTA_S**2 * V_S + SIGMA_Z_EPS**2\n",
    "V_X_analytical = TRUE_GAMMA_S**2 * V_S + SIGMA_X_EPS**2\n",
    "Cov_ZS_analytical = TRUE_DELTA_S * V_S\n",
    "Cov_XS_analytical = TRUE_GAMMA_S * V_S\n",
    "Cov_ZX_analytical = TRUE_DELTA_S * TRUE_GAMMA_S * V_S\n",
    "\n",
    "# Denominator for Naive OVB (analytical)\n",
    "Delta_naive_analytical = V_Z_analytical * V_X_analytical - Cov_ZX_analytical**2\n",
    "\n",
    "for _ in tqdm(range(N_SIMULATIONS), desc=\"Running Linear Simulations\"):\n",
    "    s, z, x, y = generate_data(N_SAMPLES)\n",
    "    \n",
    "    # Add intercept for statsmodels OLS (for regressors that might be used alone or in combinations)\n",
    "    X_s_only = sm.add_constant(s, prepend=True, has_constant='add')\n",
    "    \n",
    "    # Create a dummy (binary) sensitive attribute for DP/CDP calculation,\n",
    "    # as these metrics are typically defined for binary groups.\n",
    "    # We'll use the median of S to binarize.\n",
    "    s_binary = (s > np.median(s)).astype(int)\n",
    "\n",
    "    # --- 1. Naive Model (Y ~ Z + X) ---\n",
    "    X_naive_model = sm.add_constant(np.column_stack((z, x)), prepend=True, has_constant='add')\n",
    "    \n",
    "    model_naive = sm.OLS(y, X_naive_model).fit()\n",
    "    y_pred_naive = model_naive.predict(X_naive_model)\n",
    "\n",
    "    naive_beta_Z = model_naive.params[1]  # Beta for Z\n",
    "    naive_beta_X = model_naive.params[2]  # Beta for X\n",
    "\n",
    "    dp_naive = calculate_demographic_parity(y_pred_naive, s_binary)\n",
    "    cdp_naive = _compute_conditional_demographic_parity(y_pred_naive, s_binary, z)\n",
    "\n",
    "    ssr_naive = np.sum((y - y_pred_naive)**2)\n",
    "    tss = np.sum((y - np.mean(y))**2)\n",
    "    r2_naive = 1 - ssr_naive / tss\n",
    "\n",
    "    # --- 2. Full Debias (Y ~ Z_res + X_res) ---\n",
    "    model_z_on_s = sm.OLS(z, X_s_only).fit()\n",
    "    z_res = z - model_z_on_s.predict(X_s_only)\n",
    "\n",
    "    model_x_on_s = sm.OLS(x, X_s_only).fit()\n",
    "    x_res = x - model_x_on_s.predict(X_s_only)\n",
    "    \n",
    "    X_full_debias = sm.add_constant(np.column_stack((z_res, x_res)), prepend=True, has_constant='add')\n",
    "    model_full_debias = sm.OLS(y, X_full_debias).fit()\n",
    "    y_pred_full_debias = model_full_debias.predict(X_full_debias)\n",
    "\n",
    "    full_debias_beta_Z_res = model_full_debias.params[1]  # Beta for Z_res\n",
    "    full_debias_beta_X_res = model_full_debias.params[2]  # Beta for X_res\n",
    "\n",
    "    dp_full_debias = calculate_demographic_parity(y_pred_full_debias, s_binary)\n",
    "    cdp_full_debias = _compute_conditional_demographic_parity(y_pred_full_debias, s_binary, z)\n",
    "\n",
    "    ssr_full = np.sum((y - y_pred_full_debias)**2)\n",
    "    r2_full = 1 - ssr_full / tss\n",
    "\n",
    "    # --- 3. Our Method (Fit Y ~ Z + X_res + S, predict with S=0) ---\n",
    "    # Residualize only X (inadmissible)\n",
    "    # Fit with S to control for direct/inadmissible effects\n",
    "    X_ours = sm.add_constant(np.column_stack((z, x_res, s)), prepend=True, has_constant='add')\n",
    "    model_ours = sm.OLS(y, X_ours).fit()\n",
    "\n",
    "    ours_beta_Z = model_ours.params[1]  # Beta for Z\n",
    "    ours_beta_X_res = model_ours.params[2]  # Beta for X_res\n",
    "    # beta_S = model_ours.params[3]  # Not reported, but = beta_S + beta_X * gamma_S\n",
    "\n",
    "    # Predict by setting S to 0 (average over marginal S)\n",
    "    X_pred = sm.add_constant(np.column_stack((z, x_res, np.zeros_like(s))), prepend=True, has_constant='add')\n",
    "    y_pred_ours = model_ours.predict(X_pred)\n",
    "\n",
    "    dp_ours = calculate_demographic_parity(y_pred_ours, s_binary)\n",
    "    cdp_ours = _compute_conditional_demographic_parity(y_pred_ours, s_binary, z)\n",
    "\n",
    "    ssr_ours = np.sum((y - y_pred_ours)**2)\n",
    "    r2_ours = 1 - ssr_ours / tss\n",
    "\n",
    "    # Store results for this simulation run\n",
    "    simulation_results.append({\n",
    "        'naive_R2': r2_naive,\n",
    "        'naive_beta_Z': naive_beta_Z,\n",
    "        'naive_beta_X': naive_beta_X,\n",
    "        'naive_DP': dp_naive,\n",
    "        'naive_CDP': cdp_naive,\n",
    "\n",
    "        'full_debias_R2': r2_full,\n",
    "        'full_debias_beta_Z_res': full_debias_beta_Z_res,\n",
    "        'full_debias_beta_X_res': full_debias_beta_X_res,\n",
    "        'full_debias_DP': dp_full_debias,\n",
    "        'full_debias_CDP': cdp_full_debias,\n",
    "\n",
    "        'ours_R2': r2_ours,\n",
    "        'ours_beta_Z': ours_beta_Z,\n",
    "        'ours_beta_X_res': ours_beta_X_res,\n",
    "        'ours_DP': dp_ours,\n",
    "        'ours_CDP': cdp_ours,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(simulation_results)\n",
    "\n",
    "summary_mean = results_df.mean()\n",
    "summary_std = results_df.std()\n",
    "\n",
    "print(\"\\n--- Simulation Results Summary (Mean ± Std Dev) ---\")\n",
    "summary_table = pd.DataFrame({'mean': summary_mean, 'std': summary_std})\n",
    "print(summary_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
